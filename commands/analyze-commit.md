---
name: analyze-commit
description: Deep analysis of a single commit with context gathering and Notion export
---

# Analyze Single Commit

Perform comprehensive analysis of a single commit, understanding the **Why**, **What**, **Impact**, and **Design** behind the changes.

## Usage

```
/analyze-commit <commit-hash>
/analyze-commit <github-url> <commit-hash>  # URL optional if project registered
```

**Examples**:
```
# After /register-oss - URL not needed!
/analyze-commit abc1234567
/analyze-commit abc1234  # Short hash OK

# Or with explicit URL
/analyze-commit https://github.com/expressjs/express abc1234567
```

## Analysis Workflow

### Phase 0: Repository Setup and Local Clone

**IMPORTANT**: This command requires a local clone of the repository for deep code analysis.

#### Step 0.1: Repository URL Resolution

```python
# If URL not provided, read from memory
if not github_url:
    current_oss = serena_mcp.read_memory("current_oss")

    if not current_oss:
        print("âš ï¸  No repository specified and no current project set")
        print("Please either:")
        print("  1. Register a project: /register-oss <github-url>")
        print("  2. Or specify URL: /analyze-commit <github-url> <commit-hash>")
        return

    github_url = current_oss["repo_url"]
    owner = current_oss["owner"]
    repo = current_oss["repo"]
    notion_oss_page_id = current_oss["notion_page_id"]
    local_repo_path = current_oss.get("local_repo_path")  # May be None

    print(f"ğŸ“¦ Using current project: {owner}/{repo}")
else:
    # URL provided, parse it
    owner, repo = parse_github_url(github_url)
    local_repo_path = None
```

#### Step 0.2: Local Clone Verification

Check if repository is cloned locally:

```python
import os
from pathlib import Path

# Determine expected clone location
if not local_repo_path:
    # Default location: ~/.claude/deep-code-reader/repos/{owner}/{repo}
    home = Path.home()
    local_repo_path = home / ".claude" / "deep-code-reader" / "repos" / owner / repo

# Check if clone exists
if not os.path.exists(local_repo_path / ".git"):
    print(f"âš ï¸  Repository not cloned locally")
    print(f"")
    print(f"This repository needs to be registered first for deep code analysis:")
    print(f"")
    print(f"  /register-oss {github_url}")
    print(f"")
    print(f"The /register-oss command will:")
    print(f"  â€¢ Clone the repository to ~/.claude/deep-code-reader/repos/{owner}/{repo}")
    print(f"  â€¢ Set up Notion database for tracking")
    print(f"  â€¢ Enable deep code analysis features")
    print(f"")
    print(f"After registration, run this command again.")
    return

print(f"âœ… Repository found: {local_repo_path}")

# Activate project in Serena MCP for symbol analysis
serena_mcp.activate_project(str(local_repo_path))
print(f"ğŸ” Serena MCP activated for deep analysis")
```

#### Step 0.3: Checkout Commit

```python
# Checkout the specific commit for analysis
import subprocess

try:
    # Save current branch
    result = subprocess.run(
        ["git", "rev-parse", "--abbrev-ref", "HEAD"],
        cwd=local_repo_path,
        capture_output=True,
        text=True
    )
    original_branch = result.stdout.strip()

    # Checkout the commit
    subprocess.run(
        ["git", "checkout", commit_hash],
        cwd=local_repo_path,
        check=True,
        capture_output=True
    )

    print(f"ğŸ“ Checked out commit: {commit_hash}")

    # Store for cleanup later
    cleanup_info = {
        "repo_path": local_repo_path,
        "original_branch": original_branch,
        "commit_hash": commit_hash
    }

except subprocess.CalledProcessError as e:
    print(f"âŒ Error checking out commit: {e}")
    return
```

### Phase 1: Commit Data Gathering (use GitHub MCP)

Fetch commit information:

```python
# Get commit details
commit_info = github_mcp.get_commit(owner, repo, commit_hash)

Required data:
- Commit hash (full & short)
- Commit message
- Author & date
- Changed files
- Diff content
- Parent commits
```

### Phase 2: Context Gathering (use GitHub MCP + Sequential Thinking)

#### 2.1 Related Issues

Extract issue references from:
- Commit message: `#123`, `fixes #456`, `closes #789`
- PR associations: If commit is part of a PR

```python
# Find related issues
issues = extract_issue_numbers(commit_message)
for issue_num in issues:
    issue_data = github_mcp.get_issue(owner, repo, issue_num)
    # Store: issue title, description, labels
```

#### 2.2 Before & After Context

Get surrounding commits:

```python
# Get commit timeline
before_commits = git log --oneline {commit}~3..{commit}~1
after_commits = git log --oneline {commit}..{commit}+2

Context includes:
- 1-2 commits before
- 1-2 commits after
- Brief description of each
```

#### 2.3 Pull Request Context (if applicable)

```python
# Find associated PR
prs = github_mcp.search_prs(query=f"commit:{commit_hash}")
if prs:
    pr_data = github_mcp.get_pr(owner, repo, pr.number)
    # Store: PR title, description, reviews, comments
```

### Phase 3: Deep Code Analysis (use GitHub MCP + Sequential Thinking + Serena MCP)

**IMPORTANT**: This phase performs **line-by-line code analysis** for deep understanding.

#### 3.1 Retrieve Complete File Contents (from Local Clone + Serena MCP)

For each changed file, get the full file content from local clone:

```python
# Get commit details from GitHub API
commit_data = github_mcp.get_commit(owner, repo, commit_hash)

files_analysis = []

for file in commit_data["files"]:
    file_path = file["filename"]
    full_path = local_repo_path / file_path

    # Get the FULL file content AFTER the change (currently checked out)
    if file["status"] != "deleted":
        file_content_after = serena_mcp.read_file(
            relative_path=file_path,
            max_answer_chars=-1  # Read entire file
        )
    else:
        file_content_after = None

    # Get the file content BEFORE the change
    if file["status"] != "added":
        # Checkout parent commit temporarily to read old content
        parent_sha = commit_data["parents"][0]["sha"]

        subprocess.run(
            ["git", "checkout", parent_sha, "--", file_path],
            cwd=local_repo_path,
            capture_output=True
        )

        file_content_before = serena_mcp.read_file(
            relative_path=file_path,
            max_answer_chars=-1
        )

        # Restore to current commit
        subprocess.run(
            ["git", "checkout", commit_hash, "--", file_path],
            cwd=local_repo_path,
            capture_output=True
        )
    else:
        file_content_before = None

    # Get the patch/diff from GitHub
    diff = file["patch"]

    # Use Serena MCP to get symbol information
    symbols_info = None
    if is_code_file(file_path) and file["status"] != "deleted":
        try:
            # Get overview of symbols in this file
            symbols_info = serena_mcp.get_symbols_overview(
                relative_path=file_path,
                max_answer_chars=-1
            )
        except Exception as e:
            print(f"âš ï¸  Could not analyze symbols in {file_path}: {e}")

    # Store all for detailed analysis
    files_analysis.append({
        "path": file_path,
        "status": file["status"],  # added, modified, deleted
        "content_after": file_content_after,
        "content_before": file_content_before,
        "diff": diff,
        "additions": file["additions"],
        "deletions": file["deletions"],
        "symbols": symbols_info  # NEW: Symbol-level information
    })

print(f"âœ… Loaded {len(files_analysis)} files for deep analysis")
```

#### 3.2 Purpose of Change (Why)

Analyze **why** this change was made:

```markdown
- Problem: What problem does this solve?
- Motivation: Why was this approach chosen?
- Background: What's the business/technical context?

Sources:
- Commit message
- Related issue descriptions
- PR discussions
- Code comments in changed files
```

#### 3.3 Detailed File Analysis

**For EACH changed file**, perform deep analysis:

```python
for file_info in files_analysis:
    file_analysis = {
        "file_path": file_info["path"],
        "file_role": "",  # Role of this file
        "change_summary": "",  # Summary of changes
        "detailed_explanation": "",  # Detailed explanation
        "code_walkthrough": []  # Line-by-line code walkthrough
    }

    # Step 1: Understand file role
    file_analysis["file_role"] = analyze_file_role(
        file_path=file_info["path"],
        content=file_info["content_after"],
        project_context=commit_data
    )

    # Step 2: Change summary
    file_analysis["change_summary"] = f"""
    - å¤‰æ›´ã‚¿ã‚¤ãƒ—: {file_info["status"]} ({file_info["additions"]}è¡Œè¿½åŠ , {file_info["deletions"]}è¡Œå‰Šé™¤)
    - ä¸»ãªå¤‰æ›´å†…å®¹: [AIãŒåˆ†æ]
    """

    # Step 3: Detailed explanation (LINE-BY-LINE)
    # Parse the diff to identify changed sections
    changed_sections = parse_diff(file_info["diff"])

    for section in changed_sections:
        section_analysis = {
            "line_range": section["line_range"],
            "change_type": section["type"],  # addition, deletion, modification
            "code_before": section["code_before"],
            "code_after": section["code_after"],
            "explanation": ""
        }

        # DEEP ANALYSIS: Explain what this code does LINE BY LINE
        section_analysis["explanation"] = analyze_code_section(
            code=section["code_after"],
            context={
                "file_path": file_info["path"],
                "file_role": file_analysis["file_role"],
                "full_content": file_info["content_after"],
                "change_intent": commit_message
            }
        )

        file_analysis["code_walkthrough"].append(section_analysis)
```

**Example of detailed analysis output**:

```markdown
### ğŸ“„ src/auth/middleware.js

**File Role**:
Provides authentication middleware to verify authentication tokens in requests. Executed before all API endpoints.

**Change Summary**:
- Change Type: modified (45 lines added, 12 deleted)
- Main Changes: Enhanced input validation and added rate limiting

---

#### ğŸ” Detailed Change Analysis

**Section 1: Enhanced Token Validation (L23-L45)**

Code Before:
```javascript
function validateToken(token) {
  return jwt.verify(token, SECRET_KEY);
}
```

Code After:
```javascript
function validateToken(token) {
  // L23: Token existence check - prevent empty strings or null
  if (!token || typeof token !== 'string') {
    throw new Error('Invalid token format');
  }

  // L27-28: Token length check - reject abnormally short/long tokens
  // Standard JWT length is around 200-300 characters
  if (token.length < 50 || token.length > 500) {
    throw new Error('Token length out of acceptable range');
  }

  // L32-36: Token format validation
  // JWT consists of 3 parts: "header.payload.signature"
  const parts = token.split('.');
  if (parts.length !== 3) {
    throw new Error('Malformed JWT token');
  }

  // L40-45: Base64 encoding validation
  // Verify each part is Base64 encoded
  try {
    parts.forEach(part => {
      Buffer.from(part, 'base64');
    });
  } catch (e) {
    throw new Error('Invalid Base64 encoding in token');
  }

  // L45: Final signature verification (same as original code)
  return jwt.verify(token, SECRET_KEY);
}
```

**Why This Change Was Needed**:
Addresses vulnerability reported in CVE-2024-1234. Fixes issue where attackers could crash server by sending malformed tokens.

**Code Behavior Details**:
1. **L23-25**: First check token existence and type. This rejects invalid inputs like `undefined` or numbers early.
2. **L27-30**: Length check prevents extremely short (brute force attack) or long (DoS attack) tokens.
3. **L32-36**: Validates basic JWT structure (3-part composition). Detects malformed format early.
4. **L40-44**: Validates Base64 encoding of each part. Catches decoding errors before they crash.
5. **L45**: After passing all validations, executes original `jwt.verify()`.

**Patterns Used**:
- **Defense in Depth**: Multiple layers of validation to prevent attacks
- **Fail Fast**: Detect problems early and return errors
- **Input Validation**: Never trust external inputs, always validate

---

**Section 2: Adding Rate Limiting (L67-L89)**

[... Similar detailed analysis ...]
```

#### 3.4 Overall Changes Summary (What)

After detailed file analysis, create overall summary:

```markdown
Changed Files:
1. **src/auth/middleware.js** (45 lines added, 12 deleted)
   - Implemented multi-layered token validation
   - Added rate limiting functionality

2. **src/auth/validator.js** (23 lines added, 5 deleted)
   - Added custom validator support

3. **test/auth.test.js** (67 lines added, 0 deleted)
   - Added comprehensive test cases
   - Improved edge case coverage

Main Technical Changes:
- Enhanced input validation (length, type, format)
- Introduced rate limiting algorithm (Token Bucket method)
- Test coverage improved from 45% to 92%
```

#### 3.5 Impact Scope (Impact)

Assess **impact** on the codebase with detailed dependency analysis:

```python
# Use Serena MCP to analyze symbol dependencies
impact_analysis = {
    "affected_modules": [],
    "breaking_changes": [],
    "api_compatibility": {},
    "performance_impact": {},
    "security_impact": {}
}

# For each changed file, find what depends on it
for file_info in files_analysis:
    # Use Serena's find_referencing_symbols to find dependencies
    if file is code file:
        symbols = serena_mcp.find_symbol(
            name_path="",  # Find all symbols in file
            relative_path=file_info["path"]
        )

        for symbol in symbols:
            references = serena_mcp.find_referencing_symbols(
                name_path=symbol["name_path"],
                relative_path=file_info["path"]
            )

            impact_analysis["affected_modules"].extend(references)

# Analyze breaking changes
for change in changed_sections:
    if is_api_change(change) and not_backward_compatible(change):
        impact_analysis["breaking_changes"].append({
            "file": change["file"],
            "change": change["description"],
            "migration": generate_migration_guide(change)
        })
```

**Example output**:

```markdown
### ğŸ—ï¸ Impact Scope

#### Affected Modules

**Direct Impact** (directly uses this code):
1. **api/routes/auth.js** (12ç®‡æ‰€ã§ä½¿ç”¨)
   - L45: `validateToken(req.headers.authorization)`
   - L67: `validateToken(sessionToken)`
   - Impact: New validation logic automatically applied, no fixes needed

2. **api/routes/user.js** (8 uses)
   - L23: `middleware.validateToken(token)`
   - Impact: Error handling improvements needed (new error types)

**Indirect Impact** (affected through dependencies):
1. **middleware/session.js**
   - Affected through auth/middleware
   - No changes to session validation flow

2. **config/security.js**
   - References rate limit configuration values
   - New configuration items needed

#### Breaking Changes

âŒ **None** - All changes maintain backward compatibility

#### API Compatibility

âœ… **Fully Compatible** - No changes to existing API signatures

Added Error Types:
- `TokenFormatError` - Invalid format
- `TokenLengthError` - Length out of range
- `RateLimitError` - Rate limit exceeded

Migration Guide:
```javascript
// Before (existing code - continues to work)
try {
  const user = validateToken(token);
} catch (err) {
  // General error handling
}

// After (recommended - handles new error types)
try {
  const user = validateToken(token);
} catch (err) {
  if (err instanceof TokenFormatError) {
    // Handle invalid format specifically
  } else if (err instanceof RateLimitError) {
    // Handle rate limit specifically
  }
  // Other error handling
}
```

#### Performance Impact

**Validation Processing**:
- Additional validation steps: +0.5ms (L23-L44 processing)
- Overall latency: 2.3ms â†’ 2.8ms (+21%)
- Trade-off: Slight delay vs significant security improvement

**Memory Usage**:
- Rate limit data structure: +2MB (per 10K users)
- Token Bucket map: O(active_users) memory

**Scalability**:
- âœ… No issues: Rate limiting can migrate to Redis (noted in comments)
- âš ï¸  Note: Redis recommended for 100K+ concurrent users

#### Security Impact

**Fixed Vulnerabilities**:
- **CVE-2024-1234** (Critical): Server crash via malformed tokens
- **CVSS Score**: 9.8 â†’ 0.0 (fully fixed)

**Added Security Features**:
1. Multi-layered input validation
2. DoS attack protection (rate limiting)
3. Anomaly detection and early rejection

**Security Test Coverage**:
- Fuzzing tests added: 10,000 patterns
- Edge cases: All covered
```

#### 3.6 Design Intent and Architecture (Design)

Understand **design** decisions with deep architectural analysis:

```python
# Use Sequential Thinking to analyze design patterns
design_analysis = {
    "patterns": [],
    "trade_offs": [],
    "alternatives": [],
    "extensibility": [],
    "architectural_decisions": []
}

# Analyze each pattern used
for file_info in files_analysis:
    patterns = identify_design_patterns(file_info["content_after"])
    design_analysis["patterns"].extend(patterns)

# Analyze trade-offs
trade_offs = analyze_trade_offs(
    before=files_before,
    after=files_after,
    metrics=["performance", "security", "maintainability", "complexity"]
)
design_analysis["trade_offs"] = trade_offs
```

**Example output**:

```markdown
### ğŸ¨ Design Intent and Architecture

#### Design Patterns Used

**1. Chain of Responsibility (è²¬ä»»é€£é–ãƒ‘ã‚¿ãƒ¼ãƒ³)**

```
Request â†’ Exists Check â†’ Type Check â†’ Length Check â†’ Format Validation â†’ Base64 Validation â†’ Signature Verification â†’ Success
          â†“              â†“             â†“                â†“                   â†“                   â†“
         Error          Error         Error            Error               Error               Error
```

Why this pattern?
- Each validation step can fail independently
- Easy to add new validation rules
- Easy to write tests (test each step individually)

**2. Fail Fast (æ—©æœŸå¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³)**

å®Ÿè£…ç®‡æ‰€: L23-L44ã®å„æ¤œè¨¼ã‚¹ãƒ†ãƒƒãƒ—

åˆ©ç‚¹:
- ãƒªã‚½ãƒ¼ã‚¹ã®ç„¡é§„ã‚’é˜²ãï¼ˆç„¡åŠ¹ãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ—©æœŸã«ãƒªã‚¸ã‚§ã‚¯ãƒˆï¼‰
- ãƒ­ã‚°ã¨ã‚¨ãƒ©ãƒ¼ãƒˆãƒ¬ãƒ¼ã‚¹ã®æ˜ç¢ºåŒ–
- ãƒ‡ãƒãƒƒã‚°ã®å®¹æ˜“ã•

**3. Defense in Depth (å¤šå±¤é˜²å¾¡)**

ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ã‚¤ãƒ¤ãƒ¼:
```
Layer 1: å­˜åœ¨ãƒ»å‹ãƒã‚§ãƒƒã‚¯     â†’ åŸºæœ¬çš„ãªç„¡åŠ¹å…¥åŠ›ã‚’æ‹’å¦
Layer 2: é•·ã•ãƒã‚§ãƒƒã‚¯          â†’ DoSæ”»æ’ƒã‚’é˜²ã
Layer 3: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¤œè¨¼      â†’ æ§‹é€ çš„ãªæ”»æ’ƒã‚’é˜²ã
Layer 4: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œè¨¼  â†’ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ”»æ’ƒã‚’é˜²ã
Layer 5: ç½²åæ¤œè¨¼              â†’ å½é€ ã‚’é˜²ã
Layer 6: ãƒ¬ãƒ¼ãƒˆåˆ¶é™            â†’ ç·å½“ãŸã‚Šæ”»æ’ƒã‚’é˜²ã
```

**4. Token Bucket (ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ )**

```javascript
// L67-L89: Token Bucketå®Ÿè£…
class TokenBucket {
  constructor(capacity, refillRate) {
    this.capacity = capacity;      // ãƒã‚±ãƒƒãƒˆã®å®¹é‡ï¼ˆæœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼‰
    this.tokens = capacity;        // ç¾åœ¨ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    this.refillRate = refillRate;  // ãƒˆãƒ¼ã‚¯ãƒ³ã®è£œå……ãƒ¬ãƒ¼ãƒˆï¼ˆæ¯ç§’ï¼‰
    this.lastRefill = Date.now();
  }

  tryConsume() {
    this.refill();  // ã¾ãšãƒˆãƒ¼ã‚¯ãƒ³ã‚’è£œå……
    if (this.tokens >= 1) {
      this.tokens -= 1;
      return true;  // ãƒªã‚¯ã‚¨ã‚¹ãƒˆè¨±å¯
    }
    return false;  // ãƒ¬ãƒ¼ãƒˆåˆ¶é™è¶…é
  }

  refill() {
    const now = Date.now();
    const timePassed = (now - this.lastRefill) / 1000;
    const tokensToAdd = timePassed * this.refillRate;
    this.tokens = Math.min(this.capacity, this.tokens + tokensToAdd);
    this.lastRefill = now;
  }
}
```

ãªãœToken Bucket?
- **æŸ”è»Ÿæ€§**: ãƒãƒ¼ã‚¹ãƒˆçš„ãªãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’è¨±å®¹
- **å…¬å¹³æ€§**: é•·æœŸçš„ã«ã¯å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå¹³ç­‰
- **åŠ¹ç‡æ€§**: O(1)ã®æ™‚é–“è¨ˆç®—é‡

ä»£æ›¿æ¡ˆã¨ã®æ¯”è¼ƒ:
- Fixed Window: ãƒãƒ¼ã‚¹ãƒˆæ”»æ’ƒã«å¼±ã„ âŒ
- Sliding Window: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤šã„ âŒ
- Token Bucket: ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„ âœ…

#### è¨­è¨ˆä¸Šã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

**1. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ vs ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**

æ±ºå®š: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚’å„ªå…ˆ
- æ¤œè¨¼ã‚¹ãƒ†ãƒƒãƒ—è¿½åŠ ã«ã‚ˆã‚Š+0.5ms ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å¢—åŠ 
- ã—ã‹ã—CVE-2024-1234 (Critical)ã‚’å®Œå…¨ä¿®æ­£
- **åˆ¤æ–­**: ã‚ãšã‹ãªé…å»¶ã¯è¨±å®¹ç¯„å›²ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¯å¿…é ˆ

**2. ãƒ¡ãƒ¢ãƒªä½¿ç”¨ vs ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**

æ±ºå®š: æ®µéšçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
- åˆæœŸå®Ÿè£…: ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªï¼ˆ+2MB/1ä¸‡ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼‰
- ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—æ™‚: Redisã¸ç§»è¡Œå¯èƒ½ï¼ˆã‚³ãƒ¼ãƒ‰ã«ã‚³ãƒ¡ãƒ³ãƒˆè¨˜è¼‰ï¼‰
- **åˆ¤æ–­**: æ—©æœŸæœ€é©åŒ–ã‚’é¿ã‘ã€å¿…è¦ã«ãªã£ã¦ã‹ã‚‰ç§»è¡Œ

**3. è¤‡é›‘æ€§ vs ä¿å®ˆæ€§**

è¿½åŠ ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰è¡Œæ•°: 68è¡Œ
- å„ã‚¹ãƒ†ãƒƒãƒ—ã«è©³ç´°ãªã‚³ãƒ¡ãƒ³ãƒˆ
- ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸92%
- **åˆ¤æ–­**: è¤‡é›‘ã•ã¯å¢—ã™ãŒã€ã‚³ãƒ¡ãƒ³ãƒˆã¨ãƒ†ã‚¹ãƒˆã§ä¿å®ˆæ€§ã‚’ç¢ºä¿

#### æ¤œè¨ã•ã‚ŒãŸä»£æ›¿æ¡ˆ

**ä»£æ›¿æ¡ˆ 1: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨ (express-rate-limit)**

ãƒ¡ãƒªãƒƒãƒˆ:
- å®Ÿè£…ãŒç°¡å˜
- å®Ÿç¸¾ã‚ã‚Š

ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ:
- ä¾å­˜é–¢ä¿‚ã®è¿½åŠ 
- ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºåˆ¶é™
- **å´ä¸‹ç†ç”±**: å­¦ç¿’ç›®çš„ã®ãŸã‚è‡ªå‰å®Ÿè£…ã‚’é¸æŠ

**ä»£æ›¿æ¡ˆ 2: ã‚ˆã‚Šå³æ ¼ãªæ¤œè¨¼ (å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ‘ãƒ¼ã‚¹)**

ãƒ¡ãƒªãƒƒãƒˆ:
- ã•ã‚‰ã«è©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ:
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ä½ä¸‹ (+2ms)
- è¤‡é›‘æ€§å¢—å¤§
- **å´ä¸‹ç†ç”±**: ã‚³ã‚¹ãƒˆå¯¾åŠ¹æœãŒä½ã„

#### å°†æ¥ã®æ‹¡å¼µæ€§

**æ‹¡å¼µãƒã‚¤ãƒ³ãƒˆ 1: ã‚«ã‚¹ã‚¿ãƒ ãƒãƒªãƒ‡ãƒ¼ã‚¿**

```javascript
// L45ã«è¿½åŠ å¯èƒ½ãªæ‹¡å¼µãƒã‚¤ãƒ³ãƒˆ
const customValidators = [];  // ãƒ—ãƒ©ã‚°ã‚¤ãƒ³å¯èƒ½

function validateToken(token, customValidators = []) {
  // ... æ—¢å­˜ã®æ¤œè¨¼ ...

  // ã‚«ã‚¹ã‚¿ãƒ ãƒãƒªãƒ‡ãƒ¼ã‚¿ã‚’å®Ÿè¡Œ
  for (const validator of customValidators) {
    validator(token);
  }

  return jwt.verify(token, SECRET_KEY);
}
```

**æ‹¡å¼µãƒã‚¤ãƒ³ãƒˆ 2: Redisç§»è¡Œ**

```javascript
// L75: ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒª â†’ Redis ã¸ã®ç§»è¡ŒãŒå®¹æ˜“
// ç¾åœ¨:
const rateLimiters = new Map();  // ãƒ¡ãƒ¢ãƒªå†…

// å°†æ¥:
const rateLimiters = new RedisTokenBucket(redisClient);
// ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¯åŒã˜ã¾ã¾
```

**æ‹¡å¼µãƒã‚¤ãƒ³ãƒˆ 3: ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†**

```javascript
// å„æ¤œè¨¼ã‚¹ãƒ†ãƒƒãƒ—ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’é€ä¿¡å¯èƒ½
metrics.increment('auth.validation.format_check');
metrics.increment('auth.validation.rate_limit');
```

#### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ä¸Šã®æ±ºå®šäº‹é …

**1. åŒæœŸ vs éåŒæœŸå‡¦ç†**

æ±ºå®š: åŒæœŸå‡¦ç†ã‚’ç¶­æŒ
- ç†ç”±: æ¤œè¨¼ã¯é«˜é€Ÿï¼ˆ<1msï¼‰ã€éåŒæœŸåŒ–ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒç„¡é§„
- å°†æ¥: Redisç§»è¡Œæ™‚ã¯ async/await ã«å¤‰æ›´

**2. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æˆ¦ç•¥**

æ±ºå®š: ä¾‹å¤–ãƒ™ãƒ¼ã‚¹ (throw Error)
- ç†ç”±: Expressã®ã‚¨ãƒ©ãƒ¼ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã¨ã®çµ±åˆ
- ä¸€è²«æ€§: æ—¢å­˜ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã¨ã®æ•´åˆæ€§

**3. è¨­å®šã®å¤–éƒ¨åŒ–**

æ±ºå®š: å®šæ•°ã‚’è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¸
```javascript
// config/auth.js ã«ç§»å‹•
module.exports = {
  TOKEN_MIN_LENGTH: 50,
  TOKEN_MAX_LENGTH: 500,
  RATE_LIMIT_CAPACITY: 100,
  RATE_LIMIT_REFILL_RATE: 10
};
```

#### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              APIãƒªã‚¯ã‚¨ã‚¹ãƒˆ                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Auth Middleware â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                             â”‚
    â–¼                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Rate    â”‚                  â”‚ Token    â”‚
â”‚ Limiter â”‚                  â”‚Validator â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚                            â”‚
     â”‚ OK                         â”‚
     â–¼                            â–¼
     â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚        â”‚ 1. å­˜åœ¨ãƒã‚§ãƒƒã‚¯     â”‚
     â”‚        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚        â”‚ 2. å‹ãƒã‚§ãƒƒã‚¯       â”‚
     â”‚        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚        â”‚ 3. é•·ã•ãƒã‚§ãƒƒã‚¯     â”‚
     â”‚        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚        â”‚ 4. ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¤œè¨¼ â”‚
     â”‚        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚        â”‚ 5. Base64æ¤œè¨¼     â”‚
     â”‚        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚        â”‚ 6. ç½²åæ¤œè¨¼        â”‚
     â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                            â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ ALL OK
                â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Route Handlerâ”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
```

### Phase 4: Console Output

Display analysis in console:

```markdown
ğŸ“Š Commit Analysis: abc1234

## ğŸ¯ å¤‰æ›´ã®æ„å›³ (Why)
Fix security vulnerability in authentication middleware (CVE-2024-1234)

Related Issues:
- #1234: Security: Auth bypass in edge case
- #1235: Enhancement: Add rate limiting

## ğŸ“ å¤‰æ›´å†…å®¹ (What)

Changed Files (3):
- src/auth/middleware.js (+45, -12)
- src/auth/validator.js (+23, -5)
- test/auth.test.js (+67, -0)

Key Changes:
- Add input validation for auth tokens
- Implement rate limiting (100 req/min)
- Add comprehensive test coverage

## ğŸ—ï¸ å½±éŸ¿ç¯„å›² (Impact)

Affected Modules:
- api/routes/* (10 files) - All routes using auth
- middleware/session.js - Session handling logic
- config/security.js - Security configuration

Risk Assessment:
âœ… No breaking changes
âœ… Backward compatible
âš ï¸  Requires config update in production

## ğŸ¨ è¨­è¨ˆæ„å›³ (Design)

Pattern: Chain of Responsibility for validation
Trade-off: +2MB memory for 10x security improvement
Extensibility: Can add custom validators via plugin

## ğŸ”— ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

Before this commit:
- abc0123: Refactor auth module structure
- abc0122: Add auth logging

After this commit:
- abc1235: Update documentation
- abc1236: Deploy to staging

Related PR: #5234 "Security hardening"
- 5 approving reviews
- Passed all CI checks
- Merged 2 days ago

---

ğŸ’¾ Exporting to Notion...
```

### Phase 5: Notion Export with Deep Analysis (use Notion MCP)

**IMPORTANT**: Export ALL detailed analysis including line-by-line code explanations.

#### 5.1 Find OSS Page

```python
# Query OSSãƒªã‚¹ãƒˆ database
oss_pages = notion_mcp.query_database(
    database_id=oss_database_id,
    filter={"property": "GitHub URL", "url": {"equals": repo_url}}
)

if not oss_pages:
    print("âš ï¸  Repository not registered. Run: /register-oss <url>")
    return

oss_page_id = oss_pages[0]["id"]
```

#### 5.2 Get Commits Database ID from Memory

```python
# Get OSS-specific commits database ID from current_oss memory
current_oss = serena_mcp.read_memory("current_oss")
commits_database_id = current_oss["commits_database_id"]
```

**Important**: Each OSS has its own Commits & PRs database. The database ID is stored in Serena memory when the OSS is registered.

#### 5.3 Create Commit Entry with Deep Analysis Content

```python
# Build detailed content blocks with ALL analysis sections
content_blocks = []

# 1. å¤‰æ›´ã®æ„å›³ (Why) - Heading 2
content_blocks.append({
    "heading_2": {"rich_text": [{"text": {"content": "ğŸ¯ å¤‰æ›´ã®æ„å›³ (Why)"}}]}
})
content_blocks.append({
    "paragraph": {"rich_text": [{"text": {"content": why_analysis_text}}]}
})

# 2. ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã®è©³ç´°è§£æ - For EACH file
for file_analysis in files_analysis:
    # File heading
    content_blocks.append({
        "heading_2": {"rich_text": [{"text": {"content": f"ğŸ“„ {file_analysis['path']}"}}]}
    })

    # File role
    content_blocks.append({
        "heading_3": {"rich_text": [{"text": {"content": "ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¹å‰²"}}]}
    })
    content_blocks.append({
        "paragraph": {"rich_text": [{"text": {"content": file_analysis["file_role"]}}]}
    })

    # Change summary
    content_blocks.append({
        "heading_3": {"rich_text": [{"text": {"content": "å¤‰æ›´ã®æ¦‚è¦"}}]}
    })
    content_blocks.append({
        "paragraph": {"rich_text": [{"text": {"content": file_analysis["change_summary"]}}]}
    })

    # Detailed code walkthrough (LINE-BY-LINE)
    content_blocks.append({
        "heading_3": {"rich_text": [{"text": {"content": "ğŸ” è©³ç´°ãªå¤‰æ›´è§£æ"}}]}
    })

    for section in file_analysis["code_walkthrough"]:
        # Section heading
        content_blocks.append({
            "heading_4": {"rich_text": [{"text": {
                "content": f"ã‚»ã‚¯ã‚·ãƒ§ãƒ³: {section['line_range']} - {section['change_type']}"
            }}]}
        })

        # Code before (if exists)
        if section["code_before"]:
            content_blocks.append({
                "paragraph": {"rich_text": [{"text": {"content": "**å¤‰æ›´å‰ã®ã‚³ãƒ¼ãƒ‰:**"}}]}
            })
            content_blocks.append({
                "code": {
                    "rich_text": [{"text": {"content": section["code_before"]}}],
                    "language": detect_language(file_analysis["path"])
                }
            })

        # Code after
        content_blocks.append({
            "paragraph": {"rich_text": [{"text": {"content": "**å¤‰æ›´å¾Œã®ã‚³ãƒ¼ãƒ‰:**"}}]}
        })
        content_blocks.append({
            "code": {
                "rich_text": [{"text": {"content": section["code_after"]}}],
                "language": detect_language(file_analysis["path"])
            }
        })

        # Detailed explanation (LINE-BY-LINE)
        content_blocks.append({
            "paragraph": {"rich_text": [{"text": {"content": "**è©³ç´°ãªè§£èª¬:**"}}]}
        })
        content_blocks.append({
            "paragraph": {"rich_text": [{"text": {"content": section["explanation"]}}]}
        })

        # Pattern used
        if section.get("patterns"):
            content_blocks.append({
                "paragraph": {"rich_text": [{"text": {"content": f"**ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³:** {section['patterns']}"}}]}
            })

# 3. å½±éŸ¿ç¯„å›² (Impact)
content_blocks.append({
    "heading_2": {"rich_text": [{"text": {"content": "ğŸ—ï¸ å½±éŸ¿ç¯„å›² (Impact)"}}]}
})
content_blocks.extend(create_impact_blocks(impact_analysis))

# 4. è¨­è¨ˆæ„å›³ã¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ (Design)
content_blocks.append({
    "heading_2": {"rich_text": [{"text": {"content": "ğŸ¨ è¨­è¨ˆæ„å›³ã¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£"}}]}
})
content_blocks.extend(create_design_blocks(design_analysis))

# 5. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ (Context)
content_blocks.append({
    "heading_2": {"rich_text": [{"text": {"content": "ğŸ”— ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"}}]}
})
content_blocks.extend(create_context_blocks(related_commits, related_issues, related_pr))

# 6. Full Diff (in toggle for reference)
content_blocks.append({
    "toggle": {
        "rich_text": [{"text": {"content": "ğŸ“‹ å®Œå…¨ãªDiff (å‚è€ƒ)"}}],
        "children": create_diff_blocks(commit_data["files"])
    }
})

# Create page in OSS-specific Commits & PRs database
commit_page = notion_mcp.create_page(
    parent={"database_id": commits_database_id},
    properties={
        "Title": f"{commit_hash_short}: {commit_message_first_line}",
        "Type": "Commit",  # NEW: Distinguish from PRs
        "Commit ID / PR No": commit_hash,
        "Comment": commit_message,
        "GitHub URL": f"{repo_url}/commit/{commit_hash}",
        "Analyzed Date": {"start": datetime.now().isoformat()},  # NEW
        "Memo": "",  # Empty for user to fill
    },
    children=content_blocks  # ALL detailed analysis
)

print(f"âœ… Exported {len(content_blocks)} content blocks to Notion")
```

**Note**:
- Each file gets its own detailed section
- Code is shown with syntax highlighting
- Line-by-line explanations are included
- Full diff is in a toggle block for reference

#### 5.4 Confirm Export

```markdown
âœ… Exported to Notion!

ğŸ“„ Notion Page: https://notion.so/commit-page-id
ğŸ”— View commit: [link to OSS page] â†’ [link to this commit]

ğŸ’¡ Tip: Add personal notes in the "Memo" field
```

### Phase 6: Cleanup and Restore

**IMPORTANT**: Return repository to original state after analysis.

```python
# Restore original branch
try:
    subprocess.run(
        ["git", "checkout", cleanup_info["original_branch"]],
        cwd=cleanup_info["repo_path"],
        check=True,
        capture_output=True
    )
    print(f"âœ… Restored to branch: {cleanup_info['original_branch']}")

except subprocess.CalledProcessError as e:
    print(f"âš ï¸  Could not restore branch: {e}")
    print(f"   Please manually run: cd {cleanup_info['repo_path']} && git checkout {cleanup_info['original_branch']}")

# Deactivate Serena project
try:
    serena_mcp.think_about_whether_you_are_done()
    print(f"âœ… Serena MCP analysis complete")
except:
    pass
```

### Phase 7: Next Commit Suggestions

After successful analysis, suggest next commits to analyze:

```python
# Get surrounding commits for context
timeline_commits = github_mcp.list_commits(
    owner=owner,
    repo=repo,
    per_page=5
)

# Filter out already analyzed commits
analyzed_commits = serena_mcp.read_memory("analyzed_commits") or []
unanalyzed = [c for c in timeline_commits if c["sha"] not in analyzed_commits]

# Mark current commit as analyzed
analyzed_commits.append(commit_hash)
serena_mcp.write_memory("analyzed_commits", analyzed_commits)
```

Display suggestions:

```markdown
---

ğŸ” Next Suggestions

Recent commits from this project:

1. ğŸ†• def5678 - Add rate limiting to API endpoints
   ğŸ“… 2025-01-14 â€¢ ğŸ‘¤ janedoe â€¢ âœï¸ 5 files
   /analyze-commit def5678

2. ğŸ†• ghi9012 - Update dependencies
   ğŸ“… 2025-01-13 â€¢ ğŸ‘¤ maintainer â€¢ âœï¸ 1 file
   /analyze-commit ghi9012

3. âœ… xyz3456 - Refactor auth module (already analyzed)

ğŸ’¡ Commands:
  /list-commits           # Browse all recent commits
  /list-prs              # Browse pull requests
  /current-oss           # Check current project

Continue your learning journey! ğŸš€
```

## Error Handling

### Repository Not Set

```
âš ï¸  No Repository Specified

You haven't specified a repository URL and no current project is set.

Options:
1. Register a project first:
   /register-oss https://github.com/expressjs/express
   /analyze-commit abc1234

2. Or specify URL directly:
   /analyze-commit https://github.com/expressjs/express abc1234

Check current project:
   /current-oss
```

### Commit Not Found

```
âŒ Error: Commit not found

Commit: abc1234
Repository: expressjs/express

Possible reasons:
- Commit hash is incorrect
- Commit was force-pushed/deleted
- Private repository without access

Please verify the commit hash.
```

### Repository Not Registered

```
âš ï¸  Repository Not Registered

Before analyzing commits, register the repository:

  /register-oss https://github.com/expressjs/express

This creates a parent entry in your OSSãƒªã‚¹ãƒˆ database.
```

### Large Diff

```
âš ï¸  Large Commit Detected

This commit changes 150+ files (12,000 lines).

Options:
1. Continue with full analysis (~5min)
2. Summary only (skip detailed analysis)
3. Cancel

> _
```

### Related Issue Not Found

```
â„¹ï¸  Context Gathering

Found issue references: #1234, #5678

âœ… #1234: Security vulnerability (loaded)
âŒ #5678: Issue not found or private

Continuing with available context...
```

## Output Format

### Console (always shown)

```markdown
ğŸ“Š Commit Analysis Complete

## Summary
- Changed: 3 files
- Added: 135 lines
- Removed: 17 lines
- Impact: Medium (10 dependent modules)

## Key Insights
ğŸ¯ Why: Security fix for CVE-2024-1234
ğŸ“ What: Auth middleware validation
ğŸ—ï¸ Impact: All API routes (backward compatible)
ğŸ¨ Design: Chain of Responsibility pattern

ğŸ”— Full analysis: https://notion.so/commit-page-id
```

### Notion (complete analysis)

Full detailed page with:
- ğŸ¯ å¤‰æ›´ã®æ„å›³
- ğŸ“ å¤‰æ›´å†…å®¹ (with code diff)
- ğŸ—ï¸ å½±éŸ¿ç¯„å›²
- ğŸ¨ è¨­è¨ˆæ„å›³
- ğŸ”— ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ (issues, commits, PR)
- ğŸ“‹ Complete diff (in toggle)

## Advanced Features

### Compare Mode

```
/analyze-commit <url> <commit1>..<commit2>
```

Analyze changes between two commits.

### Focus Analysis

```
/analyze-commit <url> <commit> --focus security
```

Focus on specific aspects:
- `security`: Security implications
- `performance`: Performance impact
- `api`: API changes
- `breaking`: Breaking changes

### Skip Export

```
/analyze-commit <url> <commit> --no-export
```

Analyze only, don't export to Notion.

## Tips

1. **Start with context**: Always check related issues first
2. **Think in layers**: Why â†’ What â†’ Impact â†’ Design
3. **Use your notes**: Fill the "Memo" field in Notion
4. **Compare commits**: Use before/after commits for better understanding
5. **Focus on intent**: Understand why, not just what changed
